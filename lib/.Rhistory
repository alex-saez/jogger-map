u = sample(U, 1, replace=TRUE, prob=rowSums(marg_post_values))
v = sample(V, 1, replace=TRUE, prob=marg_post_values[U==u,])
u = u + runif(1,-.015,.015); v = v + runif(1,-.02,.02) # add jitter for plotting
uv_samples[s,] = c(u,v)
# transform back to a,b:
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
# draw θj according to θj|a,b,yj ∼ Beta(a+yj, b+nj−yj):
for(j in 1:10){
theta = rbeta(1,a+y[j],b+n[j]-y[j])
theta_samples[s,j] = theta
}
}
plot(uv_samples, xlim=c(-3,0), ylim=c(1,5), pch='.',cex=2, xlab="log(alpha/beta)", ylab="log(alpha+beta)", main"Hyperparameter simulations")
uv_samples = matrix(0,S,2)
theta_samples = matrix(0,S,10)
u = sample(U, 1, replace=TRUE, prob=rowSums(marg_post_values))
v = sample(V, 1, replace=TRUE, prob=marg_post_values[U==u,])
u
v
s
c(u,v)
uv_samples[1,]
uv_samples[1,] = c(u,v)
uv_samples[1,]
set.seed(1)
S=1000
uv_samples = matrix(0,S,2)
theta_samples = matrix(0,S,10)
for(s in 1:S){
# draw u,v from the marginal posterior
u = sample(U, 1, replace=TRUE, prob=rowSums(marg_post_values))
v = sample(V, 1, replace=TRUE, prob=marg_post_values[U==u,])
u = u + runif(1,-.015,.015); v = v + runif(1,-.02,.02) # add jitter for plotting
uv_samples[s,] = c(u,v)
# transform back to a,b:
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
# draw θj according to θj|a,b,yj ∼ Beta(a+yj, b+nj−yj):
for(j in 1:10){
theta = rbeta(1,a+y[j],b+n[j]-y[j])
theta_samples[s,j] = theta
}
}
plot(uv_samples, xlim=c(-3,0), ylim=c(1,5), pch='.',cex=2, xlab="log(alpha/beta)", ylab="log(alpha+beta)", main"Hyperparameter simulations")
plot(uv_samples, xlim=c(-3,0), ylim=c(1,5), pch='.',cex=2, xlab="log(alpha/beta)", ylab="log(alpha+beta)", main="Hyperparameter simulations")
theta_samples = apply(theta_samples,2,sort)
median = theta_samples[500,]
lower_95 = theta_samples[25,] # lower limit of 95% interval
upper_95 = theta_samples[976,] # upper limit
obs_rate = y/n
plot(obs_rate, median, xlim=c(0,.6), ylim=c(0,.6), xlab="Observed rate y(j)/n(j)", ylab="95% Posterior interval for theta(j)")
for(j in 1:10)
lines(c(obs_rate[j], obs_rate[j]), c(lower_95[j],upper_95[j]))
lines(c(0,.6),c(0,.6),lty=2)
n
obs_rate
u = uv_samples[,1]
v = uv_samples[,2]
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
u = uv_samples[,1]
v = uv_samples[,2]
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
sort(a/(a+b))[c(25,976)]
sort(rowMeans(theta_samples))[c(25,976)]
y_predic = integer(0,100)
y_predic = integer(100)
y_predic = integer(1000)
for(s in 1:1000){
# sample once from the marginal posterior
u = sample(U, 1,prob=rowSums(marg_post_values))
v = sample(V, 1, prob=marg_post_values[U==u,])
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
# draw theta from Beta(a,b)
theta = rbeta(1,a,b)
# draw y from Binomial(100,theta)
y_predic[s] = rbinom(1,100,theta)
}
sort(y_predic)[c(25,976)]
set.seed(1)
y_predic = integer(1000)
for(s in 1:1000){
# sample once from the marginal posterior
u = sample(U, 1,prob=rowSums(marg_post_values))
v = sample(V, 1, prob=marg_post_values[U==u,])
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
# draw theta from Beta(a,b)
theta = rbeta(1,a,b)
# draw y from Binomial(100,theta)
y_predic[s] = rbinom(1,100,theta)
}
sort(y_predic)[c(25,976)]
set.seed(1)
y_predic = integer(1000)
for(s in 1:1000){
# sample once from the marginal posterior
u = sample(U, 1,prob=rowSums(marg_post_values))
v = sample(V, 1, prob=marg_post_values[U==u,])
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
# draw theta from Beta(a,b)
theta = rbeta(1,a,b)
# draw y from Binomial(100,theta)
y_predic[s] = rbinom(1,100,theta)
}
sort(y_predic)[c(25,976)]
set.seed(2)
y_predic = integer(1000)
for(s in 1:1000){
# sample once from the marginal posterior
u = sample(U, 1,prob=rowSums(marg_post_values))
v = sample(V, 1, prob=marg_post_values[U==u,])
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
# draw theta from Beta(a,b)
theta = rbeta(1,a,b)
# draw y from Binomial(100,theta)
y_predic[s] = rbinom(1,100,theta)
}
sort(y_predic)[c(25,976)]
set.seed(10)
y_predic = integer(1000)
for(s in 1:1000){
# sample once from the marginal posterior
u = sample(U, 1,prob=rowSums(marg_post_values))
v = sample(V, 1, prob=marg_post_values[U==u,])
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
# draw theta from Beta(a,b)
theta = rbeta(1,a,b)
# draw y from Binomial(100,theta)
y_predic[s] = rbinom(1,100,theta)
}
sort(y_predic)[c(25,976)]
set.seed(1)
y_predic = integer(1000)
for(s in 1:1000){
# sample once from the marginal posterior
u = sample(U, 1,prob=rowSums(marg_post_values))
v = sample(V, 1, prob=marg_post_values[U==u,])
a = exp(u+v)/(1+exp(u))
b = exp(v)/(1+exp(u))
# draw theta from Beta(a,b)
theta = rbeta(1,a,b)
# draw y from Binomial(100,theta)
y_predic[s] = rbinom(1,100,theta)
}
sort(y_predic)[c(25,976)]
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
setwd("/users/rebeccalex/")
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
getwd()
source('~/.active-rstudio-document', echo=TRUE)
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
getwd()
load('~/Dropbox/NYT project/data/database.RData')
database = scrape_NYTimes(database)
source('~/Dropbox/NYT project/code/add_to_log.R', echo=TRUE)
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
load('~/Dropbox/NYT project/data/database.RData')
database = scrape_NYTimes(database)
save(database,'~/Dropbox/NYT project/data/database.RData')
save(database,file='~/Dropbox/NYT project/data/database.RData')
database = scrape_NYTimes(database)
section = "http://www.nytimes.com/services/xml/rss/nyt/pop_top.xml"
page = html(section)
mostemailed = html_nodes(page, css = "guid")
section = "http://www.nytimes.com/services/xml/rss/nyt/pop_top.xml"
page = html(section)
links = html_text(html_nodes(page, css = "guid"))
links
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
database = scrape_NYTimes(database)
database = scrape_NYTimes(database)
source('~/Dropbox/NYT project/code/add_article.R', echo=TRUE)
load('~/Dropbox/NYT project/data/database.RData')
database = scrape_NYTimes(database)
source('~/Dropbox/NYT project/code/add_article.R', echo=TRUE)
source('~/Dropbox/NYT project/code/add_article.R', echo=TRUE)
source('~/Dropbox/NYT project/code/main2.R', echo=TRUE)
load('~/Dropbox/NYT project/data/database.RData')
database = scrape_NYTimes(database)
save(database,file='~/Dropbox/NYT project/data/database.RData')
database$content[500]
database[500,]
database = scrape_NYTimes(database)
save(database,file='~/Dropbox/NYT project/data/database.RData')
database = scrape_NYTimes(database)
section = "http://www.nytimes.com/services/xml/rss/nyt/pop_top.xml"
page = html(section)
links = html_text(html_nodes(page, css = "guid"))
links
save(database,file='~/Dropbox/NYT project/data/database.RData')
section = "http://rss.nytimes.com/services/xml/rss/nyt/Travel.xml"
page = html(section)
links = html_text(html_nodes(page, css = "guid"))
links
database = scrape_NYTimes(database)
database = scrape_NYTimes(database)
save(database,file='~/Dropbox/NYT project/data/database.RData')
link="http://www.wsj.com/articles/academic-researchers-find-lucrative-work-as-big-data-scientists-1407543088"
page = html_session(link)
library(rvest)
page = html_session(link)
content = html_text(html_nodes(page,"p"))
content
load('~/Dropbox/NYT project/data/database.RData')
database$most_shared
database$most_shared[database$most_shared!=0]
database$most_viewed[database$most_viewed!=0]
database$most_emailed[database$most_emailed!=0]
max=database$most_emailed)
max(database$most_emailed)
database$most_emailed[database$most_emailed==max(database$most_emailed)]
database$title[database$most_emailed==max(database$most_emailed)]
database$title[database$most_emailed==1614.87179]
database$title[which.min(database$most_emailed-1614.87179)]
database$title[which.min(abs(database$most_emailed-1614.87179))]
database$date[database$most_shared!=0]
database$date_in[database$most_shared!=0]
database$title[database$most_shared!=0]
database$title[database$most_emailed!=0]
database$title[database$most_emailed>300]
database$title[database$most_emailed>500]
database$title[database$most_emailed>1000]
database$title[database$most_emailed>2000]
database$title[database$most_emailed>1500]
database$title[database$most_emailed>1400]
database$title[database$most_emailed>1300]
database$title[database$most_emailed>1200]
section="http://www.nytimes.com/pages/us/"
page = html(section)
library(rvest)
page = html(section)
links = html_attr(html_nodes(page, "h2 a, h3 a"),"href")
links
section="http://www.nytimes.com/pages/dining/"
page = html(section)
library(rvest)
page = html(section)
links = html_attr(html_nodes(page, "h1 a, h2 a, h3 a"),"href")
links
y = c(10,10,12,11,9)
mus = seq(8, 13, length.out = 100)
logsigmas = seq(-1, 2, length.out = 100)
posterior_a = matrix(0, 100, 100)
posterior_b = matrix(0, 100, 100)
for(m in 1:100)
for(s in 1:100){
mu = mus[m]
sigma = exp(logsigmas[s])
posterior_a[m,s] = prod(dnorm(y,mu,sigma))
posterior_b[m,s] = prod(pnorm(y+.5,mu,sigma) - pnorm(y-.5,mu,sigma))
}
posterior_a = exp(log(posterior_a)-max(log(posterior_a))) # normalize
posterior_b = exp(log(posterior_b)-max(log(posterior_b))) # normalize
# contour plots for the 2 posteriors
levls = seq(.05,.95,.1)
par(mfrow = c(1,2))
contour(mus, logsigmas, posterior_a, levels=levls, drawlabels=FALSE, xlab = "mu", ylab = "log sigma", main="Posterior a")
contour(mus, logsigmas, posterior_b, levels=levls, drawlabels=FALSE, xlab = "mu", ylab = "log sigma", main="Posterior b")
# plot difference correct-incorrect
par(mfrow = c(1,1))
image(mus, logsigmas, posterior_b-posterior_a, col=gray.colors(20), xlab = "mu", ylab = "log sigma", main="Difference b-a")
post_a_mu = rowSums(posterior_a)
post_a_logsigma = colSums(posterior_a)
post_b_mu = rowSums(posterior_b)
post_b_logsigma = colSums(posterior_b)
integrate_h_x = function(h_x, p_x, x){sum(1/2*(p_x[-1]+p_x[-length(x)])*1/2*(h_x[-1]+h_x[-length(x)])*diff(x))/
sum(1/2*(p_x[-1]+p_x[-length(x)])*diff(x))}
mean_mu_a = integrate_h_x(mus, post_a_mu, mus)
mean_mu_a
x=seq(0,100,.1)
h = dnorm(x,43.5,16)
p = dnorm(x,43.5,16)
integrate_h_x(x,p,x)
integrate_h_x(x,p,x)
integrate_h_x2 = function(h_x, p_x, x){sum(p_x*h_x*diff(x))/
sum(sum(p_xdiff(x))}
integrate_h_x2 = function(h_x, p_x, x){sum(p_x*h_x*diff(x))/sum(p_xdiff(x))}
integrate_h_x2(x,p,x)
x=seq(0,100,.01)
integrate_h_x(x,p,x)
p = dnorm(x,43.5,16)
integrate_h_x(x,p,x)
x=seq(0,100,.1)
p = dnorm(x,43.5,16)
integrate_h_x(x,p,x)
m = integrate_h_x(x,p,x)
integrate_h_x((x-m)^2,p,x)
sqrt(integrate_h_x((x-m)^2,p,x))
S=10000
mu_sim = numeric(S)
sigma_sim = numeric(S)
for(s in 1:S){
i = sample(1:100,1,prob=post_a_mu)
mu_sim[s] = mus[i]
j = sample(1:100,1,prob=posterior_a[i,])
sigma_sim[s] = exp(logsigmas[j])
}
mean(mu_sample)
mean(sigma_sample)
mean(mu_sim)
mean(sigma_sim)
mean(mu_sim)
sd(mu_sim)
mean(sigma_sim)
sd(sigma_sim)
mean_mu_a = integrate_h_x(mus, post_a_mu, mus)
sd_mu_a = sqrt(integrate_h_x(mus^2, post_a_mu, mus) - mean_mu_a^2)
mean_logsigma_a = integrate_h_x(logsigmas, post_a_logsigma, logsigmas)
mean_sigma_a = exp(mean_logsigma_a)
sd_logsigma_a = sqrt(integrate_h_x(logsigmas^2, post_a_logsigma, logsigmas) - mean_logsigma_a^2)
sd_sigma_a = exp(sd_logsigma_a)
mean_mu_a
sd_mu_a
mean_sigma_a
sd_sigma_a
mean_sigma_a = integrate_h_x(exp(logsigmas), post_a_logsigma, logsigmas)
mean_sigma_a
sd_mu_a = sqrt(integrate_h_x((mus-mean_mu_a)^2, post_a_mu, mus) )
sd_mu_a
sd_sigma_a = sqrt(integrate_h_x((exp(logsigmas)-mean_sigma_a)^2, post_a_logsigma, logsigmas))
sd_sigma_a
sd(sigma_sim)
jkhkhg
mean_mu_a
mean(mu_sim)
sd_mu_a
sd(mu_sim)
mean_sigma_a
mean(sigma_sim)
sd_sigma_a
sd(sigma_sim)
library(tm)
load(file = "./C_tweets.RData") # load vector of Clinton-related tweets
load(file = "./T_tweets.RData") # load vector of Trump-related tweets
C_tweets = unique(C_tweets) # remove duplicates
T_tweets = unique(T_tweets)
sourc = VectorSource(C_tweets)
corp_C = VCorpus(sourc) # Clinton corpus
sourc = VectorSource(T_tweets)
corp_T = VCorpus(sourc) # Trump corpus
sourc = VectorSource(c(C_tweets, T_tweets))
corp_B = VCorpus(sourc) # corpus for both
sourc = VectorSource(C_tweets)
library(tm)
load(file = "./C_tweets.RData") # load vector of Clinton-related tweets
setwd("/Users/Alex/Dropbox/Data incubator/")
setwd("/Users/Alex/Dropbox/")
setwd("/Users/Alex/")
install.packages(shiny)
install.packages('shiny')
library(shiny)
runExample("01_hello")
install.packages('choroplethrZip')
install_github('arilamstein/choroplethrZip@v1.4.0')
install.packages("choroplethr")
library(devtools)
install.packages("devtools")
library(devtools)
install_github('arilamstein/choroplethrZip@v1.4.0')
install_github("arilamstein/choroplethrCaCensusTract@v1.1.0")
library(choroplethrZip)
library(ggmap)
library(ggplot2)
library(ggmap)
get_points_form_segment = function(seg_id, blocks){
# extract points necessary for drawing the segment
library(dplyr)
seg = blocks %>% filter(PHYSICALID == seg_id) %>% select(the_geom)
seg = gsub("LINESTRING \\(", "", seg)
seg = gsub("\\)", "", seg)
points = seg %>%
strsplit(', ') %>%
unlist() %>%
strsplit(' ') %>%
unlist() %>%
as.numeric()
point_coords = data.frame(lon = points[seq(1, length(points), 2)],
lat = points[seq(2, length(points), 2)])
return(point_coords)
}
s = 134926
seg_id = s
seg = blocks %>% filter(PHYSICALID == seg_id) %>% select(the_geom)
library(dplyr)
s = 134926
seg_id = s
seg = blocks %>% filter(PHYSICALID == seg_id) %>% select(the_geom)
blocks = fread('../output/blocks_Manhattan.csv')
library(data.table)
blocks = fread('../output/blocks_Manhattan.csv')
setwd("~/Documents/ADS/Fall2016-Proj2-grp10/lib")
blocks = fread('../output/blocks_Manhattan.csv')
seg = blocks %>% filter(PHYSICALID == seg_id) %>% select(the_geom)
seg
length(seg)
blocks$the_geom[blocks$PHYSICALID==seg_id]
length(blocks$the_geom[blocks$PHYSICALID==seg_id])
nrow(seg)
which(blocks$PHYSICALID==seg_id)
length(duplicated(blocks$PHYSICALID))
length(unique(blocks$PHYSICALID))
sum(duplicated(blocks$PHYSICALID))
blocks = fread('../data/blocks.csv')
blocks0 = fread('../data/street blocks.csv')
sum(duplicated(blocks0$PHYSICALID))
seg
get_points_form_segment = function(the_geom){
# extract points necessary for drawing the segment
# Input: 'the_geom' field of a segment
library(dplyr)
the_geom = gsub("LINESTRING \\(", "", the_geom)
the_geom = gsub("\\)", "", the_geom)
points = the_geom %>%
strsplit(', ') %>%
unlist() %>%
strsplit(' ') %>%
unlist() %>%
as.numeric()
point_coords = data.frame(lon = points[seq(1, length(points), 2)],
lat = points[seq(2, length(points), 2)])
return(point_coords)
}
i=ind[1]
ind = which(blocks$PHYSICALID==seg_id).
seg_id
ind = which(blocks$PHYSICALID==seg_id)
i=ind[1]
get_points_form_segment(blocks$the_geom[i])
seg_points = get_points_form_segment(blocks$the_geom[i])
ind = which(blocks$PHYSICALID==seg_id)
m <- leaflet() %>% addTiles()
for(i in ind){
seg_points = get_points_form_segment(blocks$the_geom[i])
m = addPolylines(m, lng = seg_points$lon, lat = seg_points$lat,   opacity = 1)
}
m
library(leaflet)
ind = which(blocks$PHYSICALID==seg_id)
m <- leaflet() %>% addTiles()
for(i in ind){
seg_points = get_points_form_segment(blocks$the_geom[i])
m = addPolylines(m, lng = seg_points$lon, lat = seg_points$lat,   opacity = 1)
}
m
which(duplicated(blocks$PHYSICALID
)
)
which(duplicated(blocks$PHYSICALID))
ind = c(which(duplicated(blocks$PHYSICALID)), which(duplicated(blocks$PHYSICALID, fromLast = T)))
ind
ind = c(which(duplicated(blocks$PHYSICALID)), which(duplicated(blocks$PHYSICALID, fromLast = T)))
m <- leaflet() %>% addTiles()
for(i in ind){
seg_points = get_points_form_segment(blocks$the_geom[i])
m = addPolylines(m, lng = seg_points$lon, lat = seg_points$lat,   opacity = 1)
}
m
ind = sample(nrow(blocks), 1000)
m <- leaflet() %>% addTiles()
for(i in ind){
seg_points = get_points_form_segment(blocks$the_geom[i])
m = addPolylines(m, lng = seg_points$lon, lat = seg_points$lat,   opacity = 1)
}
m
ind = sample(nrow(blocks))
m <- leaflet() %>% addTiles()
for(i in ind){
seg_points = get_points_form_segment(blocks$the_geom[i])
m = addPolylines(m, lng = seg_points$lon, lat = seg_points$lat,   opacity = 1)
}
m
get_points_from_segment = function(the_geom){
# Extract points necessary for plotting segment on map
# Input: 'the_geom' field of a segment
library(dplyr)
the_geom = gsub("LINESTRING \\(", "", the_geom)
the_geom = gsub("\\)", "", the_geom)
points = the_geom %>%
strsplit(', ') %>%
unlist() %>%
strsplit(' ') %>%
unlist() %>%
as.numeric()
point_coords = data.frame(lon = points[seq(1, length(points), 2)],
lat = points[seq(2, length(points), 2)])
return(point_coords)
}
ind = sample(nrow(blocks), 100)
m <- leaflet() %>% addTiles()
for(i in ind){
seg_points = get_points_from_segment(blocks$the_geom[i])
m = addPolylines(m, lng = seg_points$lon, lat = seg_points$lat,   opacity = 1)
}
m
ind = sample(nrow(blocks), 500)
m <- leaflet() %>% addTiles()
for(i in ind){
seg_points = get_points_from_segment(blocks$the_geom[i])
m = addPolylines(m, lng = seg_points$lon, lat = seg_points$lat,   opacity = 1)
}
m
